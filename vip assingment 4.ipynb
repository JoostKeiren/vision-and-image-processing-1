{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subset of categories: cougar_face, helicopter, pizza, helicopter\n",
    "\n",
    "- select a set of training images\n",
    "- extract SIFT features from training images (ignore position, orientation and scale)\n",
    "- Concatenate features into matrix, one descriptor per row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roland's understanding and explination of how this system will work:\n",
    "\n",
    "SIFT calculates keypoints (e.g. 100+) for a grayscale image based on black magic we dont need to know. For each keypoint it calculates a descriptor. \n",
    "\n",
    "A descriptor is a 128-dim vector that represents 8 gradient orientations of 4x4 subgrid pixels around the pixel at a keypoint.\n",
    "\n",
    "We dont care about the keypoint info here and also dont need to track which descriptors are for which image, when building the model.\n",
    "\n",
    "This huge list of large dim vectors is (unsupervised & unlabled) clustered into groups by similarity of their 128-dim vector values. K-means clusering does the magic for us here. So each cluster contains a bunch of these vectors, that dont appear in the other clusters.\n",
    "\n",
    "Each cluster can then be thought of as a unit of some unknown, abstract \"meaning\" that is different from the others - all the descriptors within it have something (vector values) in common.\n",
    "The literature calls these units/clusters \"visual words\" as an analogy to NLP. So then we can create a \"vocabulary\" or collection of these k clusters.\n",
    "\n",
    "#### The **main idea** is that a new given image can be analyzed by its descriptors, then count for each cluster how many of those descriptors fall into that cluster - a histogram. The training images would have their histograms pre-calculated and so we can compare all images with the given by their histograms, using Bhattacharyya distance or Kullback-Leibler divergence algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans # https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# built-in\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv2.SIFT_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images - train: 199, test: 23\n"
     ]
    }
   ],
   "source": [
    "# img_paths = glob('Images/*/*.jpg')[:2] #for testing, select subset\n",
    "img_paths = glob('Images/*/*.jpg')\n",
    "train_imgs, test_img = train_test_split(img_paths, test_size=0.1, shuffle=True)\n",
    "del img_paths\n",
    "print(f'Images - train: {len(train_imgs)}, test: {len(test_img)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 1 Images\\helicopter\\image_0047.jpg ...\n",
      "Processing image 2 Images\\pizza\\image_0021.jpg ...\n",
      "Processing image 3 Images\\cougar_face\\image_0061.jpg ...\n",
      "Processing image 4 Images\\pizza\\image_0036.jpg ...\n",
      "Processing image 5 Images\\cougar_face\\image_0047.jpg ...\n",
      "Processing image 6 Images\\gramophone\\image_0006.jpg ...\n",
      "Processing image 7 Images\\cougar_face\\image_0024.jpg ...\n",
      "Processing image 8 Images\\cougar_face\\image_0021.jpg ...\n",
      "Processing image 9 Images\\helicopter\\image_0087.jpg ...\n",
      "Processing image 10 Images\\pizza\\image_0050.jpg ...\n",
      "Processing image 11 Images\\cougar_face\\image_0059.jpg ...\n",
      "Processing image 12 Images\\helicopter\\image_0043.jpg ...\n",
      "Processing image 13 Images\\cougar_face\\image_0039.jpg ...\n",
      "Processing image 14 Images\\helicopter\\image_0079.jpg ...\n",
      "Processing image 15 Images\\pizza\\image_0002.jpg ...\n",
      "Processing image 16 Images\\pizza\\image_0048.jpg ...\n",
      "Processing image 17 Images\\cougar_face\\image_0043.jpg ...\n",
      "Processing image 18 Images\\cougar_face\\image_0008.jpg ...\n",
      "Processing image 19 Images\\helicopter\\image_0053.jpg ...\n",
      "Processing image 20 Images\\cougar_face\\image_0016.jpg ...\n",
      "Processing image 21 Images\\pizza\\image_0019.jpg ...\n",
      "Processing image 22 Images\\cougar_face\\image_0054.jpg ...\n",
      "Processing image 23 Images\\helicopter\\image_0054.jpg ...\n",
      "Processing image 24 Images\\gramophone\\image_0012.jpg ...\n",
      "Processing image 25 Images\\cougar_face\\image_0033.jpg ...\n",
      "Processing image 26 Images\\gramophone\\image_0036.jpg ...\n",
      "Processing image 27 Images\\pizza\\image_0013.jpg ...\n",
      "Processing image 28 Images\\cougar_face\\image_0004.jpg ...\n",
      "Processing image 29 Images\\pizza\\image_0004.jpg ...\n",
      "Processing image 30 Images\\gramophone\\image_0007.jpg ...\n",
      "Processing image 31 Images\\gramophone\\image_0042.jpg ...\n",
      "Processing image 32 Images\\helicopter\\image_0035.jpg ...\n",
      "Processing image 33 Images\\gramophone\\image_0029.jpg ...\n",
      "Processing image 34 Images\\cougar_face\\image_0026.jpg ...\n",
      "Processing image 35 Images\\cougar_face\\image_0069.jpg ...\n",
      "Processing image 36 Images\\helicopter\\image_0081.jpg ...\n",
      "Processing image 37 Images\\pizza\\image_0011.jpg ...\n",
      "Processing image 38 Images\\gramophone\\image_0023.jpg ...\n",
      "Processing image 39 Images\\pizza\\image_0007.jpg ...\n",
      "Processing image 40 Images\\cougar_face\\image_0062.jpg ...\n",
      "Processing image 41 Images\\pizza\\image_0043.jpg ...\n",
      "Processing image 42 Images\\cougar_face\\image_0006.jpg ...\n",
      "Processing image 43 Images\\cougar_face\\image_0048.jpg ...\n",
      "Processing image 44 Images\\pizza\\image_0031.jpg ...\n",
      "Processing image 45 Images\\cougar_face\\image_0028.jpg ...\n",
      "Processing image 46 Images\\gramophone\\image_0020.jpg ...\n",
      "Processing image 47 Images\\pizza\\image_0022.jpg ...\n",
      "Processing image 48 Images\\gramophone\\image_0025.jpg ...\n",
      "Processing image 49 Images\\helicopter\\image_0046.jpg ...\n",
      "Processing image 51 Images\\cougar_face\\image_0038.jpg ...\n",
      "Processing image 52 Images\\helicopter\\image_0083.jpg ...\n",
      "Processing image 53 Images\\gramophone\\image_0010.jpg ...\n",
      "Processing image 54 Images\\helicopter\\image_0025.jpg ...\n",
      "Processing image 55 Images\\gramophone\\image_0038.jpg ...\n",
      "Processing image 56 Images\\cougar_face\\image_0018.jpg ...\n",
      "Processing image 57 Images\\cougar_face\\image_0052.jpg ...\n",
      "Processing image 58 Images\\cougar_face\\image_0010.jpg ...\n",
      "Processing image 59 Images\\gramophone\\image_0021.jpg ...\n",
      "Processing image 60 Images\\gramophone\\image_0037.jpg ...\n",
      "Processing image 61 Images\\cougar_face\\image_0050.jpg ...\n",
      "Processing image 62 Images\\helicopter\\image_0044.jpg ...\n",
      "Processing image 63 Images\\cougar_face\\image_0067.jpg ...\n",
      "Processing image 64 Images\\helicopter\\image_0041.jpg ...\n",
      "Processing image 65 Images\\cougar_face\\image_0031.jpg ...\n",
      "Processing image 66 Images\\cougar_face\\image_0013.jpg ...\n",
      "Processing image 67 Images\\helicopter\\image_0008.jpg ...\n",
      "Processing image 68 Images\\pizza\\image_0018.jpg ...\n",
      "Processing image 69 Images\\pizza\\image_0032.jpg ...\n",
      "Processing image 70 Images\\cougar_face\\image_0068.jpg ...\n",
      "Processing image 71 Images\\helicopter\\image_0045.jpg ...\n",
      "Processing image 72 Images\\cougar_face\\image_0030.jpg ...\n",
      "Processing image 73 Images\\gramophone\\image_0050.jpg ...\n",
      "Processing image 74 Images\\helicopter\\image_0024.jpg ...\n",
      "Processing image 75 Images\\helicopter\\image_0022.jpg ...\n",
      "Processing image 76 Images\\helicopter\\image_0036.jpg ...\n",
      "Processing image 77 Images\\pizza\\image_0051.jpg ...\n",
      "Processing image 78 Images\\helicopter\\image_0040.jpg ...\n",
      "Processing image 79 Images\\gramophone\\image_0040.jpg ...\n",
      "Processing image 80 Images\\cougar_face\\image_0025.jpg ...\n",
      "Processing image 81 Images\\pizza\\image_0033.jpg ...\n",
      "Processing image 82 Images\\cougar_face\\image_0019.jpg ...\n",
      "Processing image 83 Images\\pizza\\image_0049.jpg ...\n",
      "Processing image 84 Images\\gramophone\\image_0017.jpg ...\n",
      "Processing image 85 Images\\pizza\\image_0024.jpg ...\n",
      "Processing image 86 Images\\helicopter\\image_0069.jpg ...\n",
      "Processing image 87 Images\\pizza\\image_0027.jpg ...\n",
      "Processing image 88 Images\\cougar_face\\image_0001.jpg ...\n",
      "Processing image 89 Images\\helicopter\\image_0018.jpg ...\n",
      "Processing image 90 Images\\cougar_face\\image_0055.jpg ...\n",
      "Processing image 91 Images\\helicopter\\image_0032.jpg ...\n",
      "Processing image 92 Images\\cougar_face\\image_0051.jpg ...\n",
      "Processing image 93 Images\\helicopter\\image_0023.jpg ...\n",
      "Processing image 94 Images\\cougar_face\\image_0064.jpg ...\n",
      "Processing image 95 Images\\gramophone\\image_0049.jpg ...\n",
      "Processing image 96 Images\\helicopter\\image_0055.jpg ...\n",
      "Processing image 97 Images\\cougar_face\\image_0022.jpg ...\n",
      "Processing image 98 Images\\gramophone\\image_0035.jpg ...\n",
      "Processing image 99 Images\\gramophone\\image_0002.jpg ...\n",
      "Processing image 101 Images\\pizza\\image_0017.jpg ...\n",
      "Processing image 102 Images\\pizza\\image_0029.jpg ...\n",
      "Processing image 103 Images\\helicopter\\image_0058.jpg ...\n",
      "Processing image 104 Images\\cougar_face\\image_0034.jpg ...\n",
      "Processing image 105 Images\\gramophone\\image_0034.jpg ...\n",
      "Processing image 106 Images\\cougar_face\\image_0020.jpg ...\n",
      "Processing image 107 Images\\cougar_face\\image_0058.jpg ...\n",
      "Processing image 108 Images\\gramophone\\image_0015.jpg ...\n",
      "Processing image 109 Images\\cougar_face\\image_0060.jpg ...\n",
      "Processing image 110 Images\\gramophone\\image_0047.jpg ...\n",
      "Processing image 111 Images\\pizza\\image_0046.jpg ...\n",
      "Processing image 112 Images\\gramophone\\image_0019.jpg ...\n",
      "Processing image 113 Images\\pizza\\image_0026.jpg ...\n",
      "Processing image 114 Images\\helicopter\\image_0068.jpg ...\n",
      "Processing image 115 Images\\cougar_face\\image_0063.jpg ...\n",
      "Processing image 116 Images\\helicopter\\image_0085.jpg ...\n",
      "Processing image 117 Images\\pizza\\image_0014.jpg ...\n",
      "Processing image 118 Images\\cougar_face\\image_0029.jpg ...\n",
      "Processing image 119 Images\\pizza\\image_0034.jpg ...\n",
      "Processing image 120 Images\\cougar_face\\image_0011.jpg ...\n",
      "Processing image 121 Images\\pizza\\image_0052.jpg ...\n",
      "Processing image 122 Images\\pizza\\image_0040.jpg ...\n",
      "Processing image 123 Images\\gramophone\\image_0041.jpg ...\n",
      "Processing image 124 Images\\helicopter\\image_0026.jpg ...\n",
      "Processing image 125 Images\\gramophone\\image_0005.jpg ...\n",
      "Processing image 126 Images\\gramophone\\image_0003.jpg ...\n",
      "Processing image 127 Images\\cougar_face\\image_0065.jpg ...\n",
      "Processing image 128 Images\\gramophone\\image_0018.jpg ...\n",
      "Processing image 129 Images\\gramophone\\image_0046.jpg ...\n",
      "Processing image 130 Images\\helicopter\\image_0020.jpg ...\n",
      "Processing image 131 Images\\pizza\\image_0044.jpg ...\n",
      "Processing image 132 Images\\gramophone\\image_0026.jpg ...\n",
      "Processing image 133 Images\\helicopter\\image_0033.jpg ...\n",
      "Processing image 134 Images\\cougar_face\\image_0049.jpg ...\n",
      "Processing image 135 Images\\helicopter\\image_0078.jpg ...\n",
      "Processing image 136 Images\\pizza\\image_0041.jpg ...\n",
      "Processing image 137 Images\\gramophone\\image_0043.jpg ...\n",
      "Processing image 138 Images\\pizza\\image_0008.jpg ...\n",
      "Processing image 139 Images\\pizza\\image_0030.jpg ...\n",
      "Processing image 140 Images\\cougar_face\\image_0017.jpg ...\n",
      "Processing image 141 Images\\cougar_face\\image_0003.jpg ...\n",
      "Processing image 142 Images\\cougar_face\\image_0053.jpg ...\n",
      "Processing image 143 Images\\gramophone\\image_0009.jpg ...\n",
      "Processing image 144 Images\\gramophone\\image_0030.jpg ...\n",
      "Processing image 145 Images\\cougar_face\\image_0057.jpg ...\n",
      "Processing image 146 Images\\cougar_face\\image_0056.jpg ...\n",
      "Processing image 147 Images\\cougar_face\\image_0036.jpg ...\n",
      "Processing image 148 Images\\helicopter\\image_0086.jpg ...\n",
      "Processing image 149 Images\\cougar_face\\image_0015.jpg ...\n",
      "Processing image 151 Images\\pizza\\image_0023.jpg ...\n",
      "Processing image 152 Images\\cougar_face\\image_0045.jpg ...\n",
      "Processing image 153 Images\\cougar_face\\image_0007.jpg ...\n",
      "Processing image 154 Images\\gramophone\\image_0024.jpg ...\n",
      "Processing image 155 Images\\pizza\\image_0016.jpg ...\n",
      "Processing image 156 Images\\pizza\\image_0006.jpg ...\n",
      "Processing image 157 Images\\helicopter\\image_0052.jpg ...\n",
      "Processing image 158 Images\\helicopter\\image_0037.jpg ...\n",
      "Processing image 159 Images\\gramophone\\image_0039.jpg ...\n",
      "Processing image 160 Images\\gramophone\\image_0004.jpg ...\n",
      "Processing image 161 Images\\cougar_face\\image_0009.jpg ...\n",
      "Processing image 162 Images\\helicopter\\image_0042.jpg ...\n",
      "Processing image 163 Images\\pizza\\image_0039.jpg ...\n",
      "Processing image 164 Images\\pizza\\image_0010.jpg ...\n",
      "Processing image 165 Images\\gramophone\\image_0028.jpg ...\n",
      "Processing image 166 Images\\gramophone\\image_0044.jpg ...\n",
      "Processing image 167 Images\\helicopter\\image_0019.jpg ...\n",
      "Processing image 168 Images\\gramophone\\image_0013.jpg ...\n",
      "Processing image 169 Images\\gramophone\\image_0014.jpg ...\n",
      "Processing image 170 Images\\cougar_face\\image_0035.jpg ...\n",
      "Processing image 171 Images\\pizza\\image_0005.jpg ...\n",
      "Processing image 172 Images\\pizza\\image_0009.jpg ...\n",
      "Processing image 173 Images\\gramophone\\image_0001.jpg ...\n",
      "Processing image 174 Images\\pizza\\image_0037.jpg ...\n",
      "Processing image 175 Images\\pizza\\image_0045.jpg ...\n",
      "Processing image 176 Images\\pizza\\image_0053.jpg ...\n",
      "Processing image 177 Images\\gramophone\\image_0033.jpg ...\n",
      "Processing image 178 Images\\helicopter\\image_0030.jpg ...\n",
      "Processing image 179 Images\\helicopter\\image_0034.jpg ...\n",
      "Processing image 180 Images\\cougar_face\\image_0040.jpg ...\n",
      "Processing image 181 Images\\cougar_face\\image_0032.jpg ...\n",
      "Processing image 182 Images\\pizza\\image_0001.jpg ...\n",
      "Processing image 183 Images\\cougar_face\\image_0023.jpg ...\n",
      "Processing image 184 Images\\helicopter\\image_0056.jpg ...\n",
      "Processing image 185 Images\\cougar_face\\image_0044.jpg ...\n",
      "Processing image 186 Images\\pizza\\image_0028.jpg ...\n",
      "Processing image 187 Images\\gramophone\\image_0027.jpg ...\n",
      "Processing image 188 Images\\helicopter\\image_0084.jpg ...\n",
      "Processing image 189 Images\\gramophone\\image_0031.jpg ...\n",
      "Processing image 190 Images\\gramophone\\image_0022.jpg ...\n",
      "Processing image 191 Images\\helicopter\\image_0057.jpg ...\n",
      "Processing image 192 Images\\helicopter\\image_0027.jpg ...\n",
      "Processing image 193 Images\\helicopter\\image_0021.jpg ...\n",
      "Processing image 194 Images\\helicopter\\image_0080.jpg ...\n",
      "Processing image 195 Images\\pizza\\image_0042.jpg ...\n",
      "Processing image 196 Images\\cougar_face\\image_0014.jpg ...\n",
      "Processing image 197 Images\\cougar_face\\image_0041.jpg ...\n",
      "Processing image 198 Images\\gramophone\\image_0048.jpg ...\n",
      "Extracted 111245 SIFT descriptors. (111245, 128)\n"
     ]
    }
   ],
   "source": [
    "# read all images and extract their descriptors\n",
    "# We also dont need to track which descriptors are for which image. We will reconstruct info like that later. Here they can all be thrown together in a big list.\n",
    "\n",
    "all_descriptors = []\n",
    "for i, jpg_path in enumerate(train_imgs):\n",
    "    if i % 50: print(f'Processing image {i} {jpg_path} ...') #progress\n",
    "\n",
    "    img = cv2.imread(jpg_path, cv2.IMREAD_GRAYSCALE)  # SIFT works on grayscale images\n",
    "    if img is not None:\n",
    "        # Detect keypoints and compute descriptors\n",
    "        keypoints, descriptors = sift.detectAndCompute(img, None)\n",
    "        # print(descriptors)\n",
    "\n",
    "        if descriptors is not None:\n",
    "            all_descriptors.append(descriptors)\n",
    "\n",
    "        # for debug, output new images with the SIFT detected interest points (descriptors)\n",
    "        if False:\n",
    "            # Draw the keypoints on the image\n",
    "            img_with_keypoints = cv2.drawKeypoints(\n",
    "                img, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n",
    "            )\n",
    "            \n",
    "            # Save the output image with keypoints\n",
    "            cat = Path(jpg_path).parts[1] #OS agnostic split\n",
    "            output_path = os.path.join('out', f\"sift_{cat}_{os.path.basename(jpg_path)}\")\n",
    "            cv2.imwrite(output_path, img_with_keypoints)\n",
    "\n",
    "# prepare them in a more convenient structure\n",
    "all_descriptors = np.vstack(all_descriptors)\n",
    "# all_descriptors = np.array(all_descriptors)\n",
    "print(f\"Extracted {all_descriptors.shape[0]} SIFT descriptors.\", all_descriptors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-4 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-4 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-4 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-4 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-4 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-4 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-4 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-4 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(n_clusters=200)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>KMeans</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.cluster.KMeans.html\">?<span>Documentation for KMeans</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>KMeans(n_clusters=200)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "KMeans(n_clusters=200)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform k-means clustering\n",
    "# this huge list of large dim vectors is (unsupervised & unlabled) clustered into groups by similarity of their 128-dim vector values\n",
    "# each cluster can then be thought of a unit of some unknown, abstract meaning that is different from the others - all the descriptors within it have something (vector values) in common\n",
    "# the literature calls these units/clusters \"visual words\" as an analogy to NLP. So then we can create a \"vocabulary\" or collection of these k clusters.\n",
    "\n",
    "k = 200 #cluster count. experimental\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "kmeans.fit(all_descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenDescriptorClusterHistogram(img_path:str):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # SIFT works on grayscale images\n",
    "    if img is not None:\n",
    "        keypoints, descriptors = sift.detectAndCompute(img, None) # Detect keypoints and compute descriptors\n",
    "        if descriptors is not None:\n",
    "            # Predict cluster indices for descriptors\n",
    "            # for each desc see which cluster it belongs to\n",
    "            cluster_indices = kmeans.predict(descriptors)\n",
    "\n",
    "            # Build histogram\n",
    "            # count how many descriptors fall into each cluster\n",
    "            return np.histogram(cluster_indices, bins=range(k + 1))[0]\n",
    "    \n",
    "    # fallback\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed BoW representations (descriptor-cluster histograms) for 199 images. (199, 200)\n"
     ]
    }
   ],
   "source": [
    "# Initialize an array to store BoW representations (descriptor-cluster histograms) for each training dataset image\n",
    "train_histograms = []\n",
    "\n",
    "for jpg_path in train_imgs:\n",
    "    hist = GenDescriptorClusterHistogram(jpg_path)\n",
    "    if hist is not None:\n",
    "        train_histograms.append(hist)\n",
    "\n",
    "train_histograms = np.array(train_histograms)\n",
    "print(f\"Constructed BoW representations (descriptor-cluster histograms) for {train_histograms.shape[0]} images.\", train_histograms.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bhattacharyya_distance(hist1, hist2):\n",
    "    # Normalize the histograms\n",
    "    hist1 = hist1 / np.sum(hist1)\n",
    "    hist2 = hist2 / np.sum(hist2)\n",
    "    \n",
    "    # Compute Bhattacharyya coefficient\n",
    "    bc = np.sum(np.sqrt(hist1 * hist2))\n",
    "    \n",
    "    # Compute Bhattacharyya distance\n",
    "    return -np.log(bc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images\\cougar_face\\image_0066.jpg top 5 similarities: [('38%', 'Images\\\\cougar_face\\\\image_0024.jpg'), ('44%', 'Images\\\\cougar_face\\\\image_0028.jpg'), ('46%', 'Images\\\\cougar_face\\\\image_0067.jpg'), ('46%', 'Images\\\\cougar_face\\\\image_0038.jpg'), ('46%', 'Images\\\\cougar_face\\\\image_0051.jpg')]\n",
      "Images\\pizza\\image_0003.jpg top 5 similarities: [('53%', 'Images\\\\cougar_face\\\\image_0054.jpg'), ('54%', 'Images\\\\pizza\\\\image_0016.jpg'), ('54%', 'Images\\\\gramophone\\\\image_0012.jpg'), ('54%', 'Images\\\\helicopter\\\\image_0052.jpg'), ('54%', 'Images\\\\cougar_face\\\\image_0035.jpg')]\n",
      "Images\\cougar_face\\image_0046.jpg top 5 similarities: [('33%', 'Images\\\\cougar_face\\\\image_0028.jpg'), ('33%', 'Images\\\\cougar_face\\\\image_0048.jpg'), ('34%', 'Images\\\\cougar_face\\\\image_0007.jpg'), ('35%', 'Images\\\\cougar_face\\\\image_0016.jpg'), ('35%', 'Images\\\\cougar_face\\\\image_0022.jpg')]\n",
      "Images\\pizza\\image_0025.jpg top 5 similarities: [('40%', 'Images\\\\cougar_face\\\\image_0028.jpg'), ('40%', 'Images\\\\pizza\\\\image_0007.jpg'), ('40%', 'Images\\\\pizza\\\\image_0016.jpg'), ('40%', 'Images\\\\cougar_face\\\\image_0064.jpg'), ('40%', 'Images\\\\cougar_face\\\\image_0048.jpg')]\n",
      "Images\\gramophone\\image_0051.jpg top 5 similarities: [('49%', 'Images\\\\pizza\\\\image_0016.jpg'), ('49%', 'Images\\\\pizza\\\\image_0008.jpg'), ('49%', 'Images\\\\pizza\\\\image_0030.jpg'), ('49%', 'Images\\\\helicopter\\\\image_0044.jpg'), ('50%', 'Images\\\\gramophone\\\\image_0009.jpg')]\n",
      "Images\\pizza\\image_0047.jpg top 5 similarities: [('45%', 'Images\\\\pizza\\\\image_0021.jpg'), ('45%', 'Images\\\\cougar_face\\\\image_0021.jpg'), ('46%', 'Images\\\\cougar_face\\\\image_0048.jpg'), ('46%', 'Images\\\\cougar_face\\\\image_0025.jpg'), ('46%', 'Images\\\\cougar_face\\\\image_0054.jpg')]\n",
      "Images\\cougar_face\\image_0037.jpg top 5 similarities: [('42%', 'Images\\\\cougar_face\\\\image_0023.jpg'), ('42%', 'Images\\\\cougar_face\\\\image_0051.jpg'), ('42%', 'Images\\\\cougar_face\\\\image_0028.jpg'), ('43%', 'Images\\\\cougar_face\\\\image_0052.jpg'), ('43%', 'Images\\\\cougar_face\\\\image_0016.jpg')]\n",
      "Images\\cougar_face\\image_0027.jpg top 5 similarities: [('48%', 'Images\\\\cougar_face\\\\image_0048.jpg'), ('50%', 'Images\\\\pizza\\\\image_0004.jpg'), ('50%', 'Images\\\\cougar_face\\\\image_0039.jpg'), ('51%', 'Images\\\\cougar_face\\\\image_0028.jpg'), ('51%', 'Images\\\\cougar_face\\\\image_0064.jpg')]\n",
      "Images\\gramophone\\image_0016.jpg top 5 similarities: [('52%', 'Images\\\\gramophone\\\\image_0023.jpg'), ('52%', 'Images\\\\gramophone\\\\image_0009.jpg'), ('52%', 'Images\\\\gramophone\\\\image_0037.jpg'), ('53%', 'Images\\\\gramophone\\\\image_0030.jpg'), ('53%', 'Images\\\\cougar_face\\\\image_0040.jpg')]\n",
      "Images\\helicopter\\image_0031.jpg top 5 similarities: [('50%', 'Images\\\\gramophone\\\\image_0034.jpg'), ('51%', 'Images\\\\helicopter\\\\image_0087.jpg'), ('51%', 'Images\\\\gramophone\\\\image_0015.jpg'), ('52%', 'Images\\\\helicopter\\\\image_0046.jpg'), ('53%', 'Images\\\\pizza\\\\image_0014.jpg')]\n",
      "Images\\helicopter\\image_0051.jpg top 5 similarities: [('60%', 'Images\\\\gramophone\\\\image_0021.jpg'), ('60%', 'Images\\\\helicopter\\\\image_0055.jpg'), ('62%', 'Images\\\\helicopter\\\\image_0033.jpg'), ('62%', 'Images\\\\helicopter\\\\image_0081.jpg'), ('63%', 'Images\\\\helicopter\\\\image_0053.jpg')]\n",
      "Images\\helicopter\\image_0009.jpg top 5 similarities: [('53%', 'Images\\\\cougar_face\\\\image_0044.jpg'), ('54%', 'Images\\\\cougar_face\\\\image_0018.jpg'), ('54%', 'Images\\\\gramophone\\\\image_0034.jpg'), ('54%', 'Images\\\\pizza\\\\image_0007.jpg'), ('54%', 'Images\\\\cougar_face\\\\image_0048.jpg')]\n",
      "Images\\gramophone\\image_0045.jpg top 5 similarities: [('44%', 'Images\\\\pizza\\\\image_0004.jpg'), ('44%', 'Images\\\\cougar_face\\\\image_0044.jpg'), ('44%', 'Images\\\\pizza\\\\image_0030.jpg'), ('45%', 'Images\\\\cougar_face\\\\image_0008.jpg'), ('46%', 'Images\\\\helicopter\\\\image_0036.jpg')]\n",
      "Images\\pizza\\image_0038.jpg top 5 similarities: [('28%', 'Images\\\\pizza\\\\image_0004.jpg'), ('29%', 'Images\\\\pizza\\\\image_0007.jpg'), ('30%', 'Images\\\\cougar_face\\\\image_0015.jpg'), ('30%', 'Images\\\\pizza\\\\image_0032.jpg'), ('30%', 'Images\\\\pizza\\\\image_0023.jpg')]\n",
      "Images\\cougar_face\\image_0005.jpg top 5 similarities: [('37%', 'Images\\\\cougar_face\\\\image_0015.jpg'), ('39%', 'Images\\\\pizza\\\\image_0004.jpg'), ('40%', 'Images\\\\cougar_face\\\\image_0007.jpg'), ('40%', 'Images\\\\cougar_face\\\\image_0022.jpg'), ('40%', 'Images\\\\pizza\\\\image_0045.jpg')]\n",
      "Images\\gramophone\\image_0011.jpg top 5 similarities: [('43%', 'Images\\\\cougar_face\\\\image_0044.jpg'), ('44%', 'Images\\\\gramophone\\\\image_0034.jpg'), ('44%', 'Images\\\\pizza\\\\image_0014.jpg'), ('45%', 'Images\\\\cougar_face\\\\image_0026.jpg'), ('45%', 'Images\\\\cougar_face\\\\image_0018.jpg')]\n",
      "Images\\pizza\\image_0015.jpg top 5 similarities: [('28%', 'Images\\\\pizza\\\\image_0016.jpg'), ('28%', 'Images\\\\pizza\\\\image_0042.jpg'), ('28%', 'Images\\\\pizza\\\\image_0004.jpg'), ('29%', 'Images\\\\pizza\\\\image_0046.jpg'), ('29%', 'Images\\\\pizza\\\\image_0030.jpg')]\n",
      "Images\\pizza\\image_0012.jpg top 5 similarities: [('34%', 'Images\\\\pizza\\\\image_0016.jpg'), ('35%', 'Images\\\\pizza\\\\image_0036.jpg'), ('35%', 'Images\\\\pizza\\\\image_0053.jpg'), ('36%', 'Images\\\\pizza\\\\image_0013.jpg'), ('36%', 'Images\\\\pizza\\\\image_0041.jpg')]\n",
      "Images\\gramophone\\image_0008.jpg top 5 similarities: [('50%', 'Images\\\\cougar_face\\\\image_0048.jpg'), ('50%', 'Images\\\\helicopter\\\\image_0087.jpg'), ('50%', 'Images\\\\pizza\\\\image_0008.jpg'), ('51%', 'Images\\\\helicopter\\\\image_0030.jpg'), ('51%', 'Images\\\\cougar_face\\\\image_0044.jpg')]\n",
      "Images\\pizza\\image_0020.jpg top 5 similarities: [('31%', 'Images\\\\pizza\\\\image_0032.jpg'), ('31%', 'Images\\\\pizza\\\\image_0022.jpg'), ('31%', 'Images\\\\pizza\\\\image_0004.jpg'), ('32%', 'Images\\\\pizza\\\\image_0030.jpg'), ('32%', 'Images\\\\pizza\\\\image_0046.jpg')]\n",
      "Images\\cougar_face\\image_0002.jpg top 5 similarities: [('36%', 'Images\\\\pizza\\\\image_0033.jpg'), ('40%', 'Images\\\\pizza\\\\image_0034.jpg'), ('40%', 'Images\\\\helicopter\\\\image_0044.jpg'), ('41%', 'Images\\\\pizza\\\\image_0043.jpg'), ('41%', 'Images\\\\cougar_face\\\\image_0059.jpg')]\n",
      "Images\\pizza\\image_0035.jpg top 5 similarities: [('29%', 'Images\\\\cougar_face\\\\image_0048.jpg'), ('32%', 'Images\\\\pizza\\\\image_0007.jpg'), ('32%', 'Images\\\\pizza\\\\image_0016.jpg'), ('32%', 'Images\\\\cougar_face\\\\image_0028.jpg'), ('32%', 'Images\\\\pizza\\\\image_0021.jpg')]\n",
      "Images\\gramophone\\image_0032.jpg top 5 similarities: [('51%', 'Images\\\\cougar_face\\\\image_0039.jpg'), ('52%', 'Images\\\\gramophone\\\\image_0020.jpg'), ('52%', 'Images\\\\helicopter\\\\image_0087.jpg'), ('53%', 'Images\\\\gramophone\\\\image_0023.jpg'), ('53%', 'Images\\\\gramophone\\\\image_0037.jpg')]\n"
     ]
    }
   ],
   "source": [
    "# Bhattacharyya distance\n",
    "\n",
    "for jpg_path in test_img:\n",
    "    test_hist = GenDescriptorClusterHistogram(jpg_path)\n",
    "    if test_hist is not None:\n",
    "        similarities = []\n",
    "        for train_img_idx, train_hist in enumerate(train_histograms):\n",
    "            # Normalize histograms to sum to 1\n",
    "            test_hist = test_hist / np.sum(test_hist)\n",
    "            train_hist = train_hist / np.sum(train_hist)\n",
    "            # opencv bs:\n",
    "            test_hist = test_hist.astype(np.float32)\n",
    "            train_hist = train_hist.astype(np.float32)\n",
    "\n",
    "            similarity = cv2.compareHist(test_hist, train_hist, cv2.HISTCMP_BHATTACHARYYA) # Bhattacharyya distance\n",
    "            similarities.append((similarity, train_img_idx))\n",
    "\n",
    "        # Convert to numpy array for sorting\n",
    "        similarities = np.array(similarities, dtype=[('similarity', float), ('index', int)])\n",
    "        # Sort by similarity\n",
    "        similarities.sort(order='similarity')\n",
    "        top_n = 3\n",
    "        similarities = similarities[:top_n]\n",
    "\n",
    "        print(f'{jpg_path} top {top_n} similarities:', list(map(lambda x: (f'{round(x[0] * 100)}%', train_imgs[x[1]]), similarities)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This needs to be reworked. Idk how its supposed to work, this is mostly GPT code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(199, 200)\n",
      "Test TF-IDF contains NaN: True\n",
      "[[-0.05143162 -0.03145919 -0.04153388 ... -0.0182731  -0.02561479\n",
      "  -0.01064249]\n",
      " [-0.07115052         nan         nan ...         nan         nan\n",
      "          nan]\n",
      " [-0.03630062 -0.02070927 -0.02418405 ... -0.00471981 -0.02087962\n",
      "  -0.01103194]\n",
      " ...\n",
      " [-0.0367247  -0.03105844 -0.04978673 ... -0.03019404 -0.01858071\n",
      "  -0.01807054]\n",
      " [-0.01429986 -0.01421659 -0.03155761 ... -0.02323757 -0.01946222\n",
      "  -0.03434632]\n",
      " [        nan         nan         nan ... -0.0850493  -0.03620327\n",
      "          nan]]\n",
      "(199, 200)\n",
      "Test TF-IDF contains NaN: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rolan\\AppData\\Local\\Temp\\ipykernel_6916\\3341633999.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  idf = np.log(train_histograms / (np.count_nonzero(train_histograms > 0, axis=0) + 1))\n",
      "C:\\Users\\rolan\\AppData\\Local\\Temp\\ipykernel_6916\\3341633999.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  training_tfidf_vectors = tf * idf\n",
      "C:\\Users\\rolan\\AppData\\Local\\Temp\\ipykernel_6916\\3341633999.py:18: RuntimeWarning: invalid value encountered in multiply\n",
      "  test_hist_tfidf = test_hist_tf * idf\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest TF-IDF contains NaN:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39misnan(test_hist_tfidf)\u001b[38;5;241m.\u001b[39many())\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Compare with training histograms' TF-IDF vectors\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m similarities \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_hist_tfidf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_tfidf_vectors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(similarities)\n",
      "File \u001b[1;32mf:\\KU\\Visual & Image processing 1\\VIP Assignments\\VIP A4\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mf:\\KU\\Visual & Image processing 1\\VIP Assignments\\VIP A4\\.venv\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:1741\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m \n\u001b[0;32m   1697\u001b[0m \u001b[38;5;124;03mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1737\u001b[0m \u001b[38;5;124;03m       [0.57..., 0.81...]])\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1739\u001b[0m \u001b[38;5;66;03m# to avoid recursive import\u001b[39;00m\n\u001b[1;32m-> 1741\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_pairwise_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1743\u001b[0m X_normalized \u001b[38;5;241m=\u001b[39m normalize(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m Y:\n",
      "File \u001b[1;32mf:\\KU\\Visual & Image processing 1\\VIP Assignments\\VIP A4\\.venv\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:200\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, ensure_all_finite, ensure_2d, copy)\u001b[0m\n\u001b[0;32m    190\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    191\u001b[0m         X,\n\u001b[0;32m    192\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    197\u001b[0m         ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m    198\u001b[0m     )\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     Y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    210\u001b[0m         Y,\n\u001b[0;32m    211\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    216\u001b[0m         ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m    217\u001b[0m     )\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m precomputed:\n",
      "File \u001b[1;32mf:\\KU\\Visual & Image processing 1\\VIP Assignments\\VIP A4\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1107\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1103\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[1;32m-> 1107\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1116\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32mf:\\KU\\Visual & Image processing 1\\VIP Assignments\\VIP A4\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:120\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\KU\\Visual & Image processing 1\\VIP Assignments\\VIP A4\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    168\u001b[0m     )\n\u001b[1;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "# TF-IDF method\n",
    "\n",
    "# Precompute TF-IDF for each histogram\n",
    "idf = np.log(train_histograms / (np.count_nonzero(train_histograms > 0, axis=0) + 1))\n",
    "tf = train_histograms / train_histograms.sum(axis=1, keepdims=True)\n",
    "training_tfidf_vectors = tf * idf\n",
    "print(training_tfidf_vectors.shape)\n",
    "print(\"Test TF-IDF contains NaN:\", np.isnan(training_tfidf_vectors).any())\n",
    "print(training_tfidf_vectors)\n",
    "\n",
    "for jpg_path in test_img[:1]:\n",
    "    test_hist = GenDescriptorClusterHistogram(jpg_path)\n",
    "    if test_hist is not None:\n",
    "        # Compute TF for the test histogram\n",
    "        test_hist_tf = test_hist / np.sum(test_hist)\n",
    "\n",
    "        # Compute TF-IDF for the test histogram\n",
    "        test_hist_tfidf = test_hist_tf * idf\n",
    "        print(test_hist_tfidf.shape)\n",
    "        print(\"Test TF-IDF contains NaN:\", np.isnan(test_hist_tfidf).any())\n",
    "\n",
    "        # Compare with training histograms' TF-IDF vectors\n",
    "        similarities = cosine_similarity(test_hist_tfidf, training_tfidf_vectors)\n",
    "        print(similarities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

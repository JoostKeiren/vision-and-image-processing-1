{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIP Assignment 4, 2025 (Rolands, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roland's understanding and explination of how this system will work:\n",
    "\n",
    "SIFT calculates keypoints (e.g. 100+) for a grayscale image based on black magic we dont need to know. For each keypoint it calculates a descriptor. \n",
    "\n",
    "A descriptor is a 128-dim vector that represents 8 gradient orientations of 4x4 subgrid pixels around the pixel at a keypoint.\n",
    "\n",
    "We dont care about the keypoint info here and also dont need to track which descriptors are for which image, when building the model.\n",
    "\n",
    "This huge list of large dim vectors is (unsupervised & unlabled) clustered into groups by similarity of their 128-dim vector values. K-means clusering does the magic for us here. So each cluster contains a bunch of these vectors, that dont appear in the other clusters.\n",
    "\n",
    "Each cluster can then be thought of as a unit of some unknown, abstract \"meaning\" that is different from the others - all the descriptors within it have something (vector values) in common.\n",
    "The literature calls these units/clusters \"visual words\" as an analogy to NLP. So then we can create a \"vocabulary\" or collection of these k clusters.\n",
    "\n",
    "#### The **main idea** is that a new given image can be analyzed by its descriptors, then count for each cluster how many of those descriptors fall into that cluster - a histogram. The training images would have their histograms pre-calculated and so we can compare all images with the given by their histograms, using Bhattacharyya distance or Kullback-Leibler divergence algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans # https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# built-in\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv2.SIFT_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_paths = glob('Images/*/*.jpg')[:2] #for testing, select subset\n",
    "img_paths = glob('Images/*/*.jpg')\n",
    "train_imgs, test_img = train_test_split(img_paths, test_size=0.005, shuffle=True)\n",
    "del img_paths\n",
    "print(f'Images - train: {len(train_imgs)}, test: {len(test_img)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all images and extract their descriptors\n",
    "# We also dont need to track which descriptors are for which image. We will reconstruct info like that later. Here they can all be thrown together in a big list.\n",
    "\n",
    "all_descriptors = []\n",
    "for i, jpg_path in enumerate(train_imgs):\n",
    "    if i % 100 == 0: print(f'Processing images {round(i*100/len(train_imgs))}% ...') #progress\n",
    "\n",
    "    img = cv2.imread(jpg_path, cv2.IMREAD_GRAYSCALE)  # SIFT works on grayscale images\n",
    "    if img is not None:\n",
    "        # Detect keypoints and compute descriptors\n",
    "        keypoints, descriptors = sift.detectAndCompute(img, None)\n",
    "        # print(descriptors)\n",
    "\n",
    "        if descriptors is not None:\n",
    "            all_descriptors.append(descriptors)\n",
    "\n",
    "        # for debug, output new images with the SIFT detected interest points (descriptors)\n",
    "        if False:\n",
    "            # Draw the keypoints on the image\n",
    "            img_with_keypoints = cv2.drawKeypoints(\n",
    "                img, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n",
    "            )\n",
    "            \n",
    "            # Save the output image with keypoints\n",
    "            cat = Path(jpg_path).parts[1] #OS agnostic split\n",
    "            output_path = os.path.join('out', f\"sift_{cat}_{os.path.basename(jpg_path)}\")\n",
    "            cv2.imwrite(output_path, img_with_keypoints)\n",
    "\n",
    "# prepare them in a more convenient structure\n",
    "all_descriptors = np.vstack(all_descriptors)\n",
    "# all_descriptors = np.array(all_descriptors)\n",
    "print(f\"Extracted {all_descriptors.shape[0]} SIFT descriptors.\", all_descriptors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform k-means clustering (this takes a few mins)\n",
    "# this huge list of large dim vectors is (unsupervised & unlabled) clustered into groups by similarity of their 128-dim vector values\n",
    "# each cluster can then be thought of a unit of some unknown, abstract meaning that is different from the others - all the descriptors within it have something (vector values) in common\n",
    "# the literature calls these units/clusters \"visual words\" as an analogy to NLP. So then we can create a \"vocabulary\" or collection of these k clusters.\n",
    "\n",
    "k = 500 #cluster count. experimental\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "kmeans.fit(all_descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenDescriptorClusterHistogram(img_path:str):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # SIFT works on grayscale images\n",
    "    if img is not None:\n",
    "        keypoints, descriptors = sift.detectAndCompute(img, None) # Detect keypoints and compute descriptors\n",
    "        if descriptors is not None:\n",
    "            # Predict cluster indices for descriptors\n",
    "            # for each desc see which cluster it belongs to\n",
    "            cluster_indices = kmeans.predict(descriptors)\n",
    "\n",
    "            # Build histogram\n",
    "            # count how many descriptors fall into each cluster\n",
    "            return np.histogram(cluster_indices, bins=range(k + 1))[0]\n",
    "    \n",
    "    # fallback\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an array to store BoW representations (descriptor-cluster histograms) for each training dataset image\n",
    "train_histograms = []\n",
    "\n",
    "for jpg_path in train_imgs:\n",
    "    hist = GenDescriptorClusterHistogram(jpg_path)\n",
    "    if hist is not None:\n",
    "        train_histograms.append(hist)\n",
    "\n",
    "train_histograms = np.array(train_histograms)\n",
    "print(f\"Constructed BoW representations (descriptor-cluster histograms) for {train_histograms.shape[0]} images.\", train_histograms.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bhattacharyya_distance(hist1, hist2):\n",
    "    # Normalize the histograms\n",
    "    hist1 = hist1 / np.sum(hist1)\n",
    "    hist2 = hist2 / np.sum(hist2)\n",
    "    \n",
    "    # Compute Bhattacharyya coefficient\n",
    "    bc = np.sum(np.sqrt(hist1 * hist2))\n",
    "    \n",
    "    # Compute Bhattacharyya distance\n",
    "    return -np.log(bc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bhattacharyya distance\n",
    "\n",
    "# random.shuffle(test_img) #so u can rerun this\n",
    "# for jpg_path in test_img[:5]:\n",
    "for jpg_path in test_img:\n",
    "    test_hist = GenDescriptorClusterHistogram(jpg_path)\n",
    "    if test_hist is not None:\n",
    "        similarities = []\n",
    "        for train_img_idx, train_hist in enumerate(train_histograms):\n",
    "            # Normalize histograms to sum to 1\n",
    "            test_hist = test_hist / np.sum(test_hist)\n",
    "            train_hist = train_hist / np.sum(train_hist)\n",
    "            # opencv bs:\n",
    "            test_hist = test_hist.astype(np.float32)\n",
    "            train_hist = train_hist.astype(np.float32)\n",
    "\n",
    "            similarity = cv2.compareHist(test_hist, train_hist, cv2.HISTCMP_BHATTACHARYYA) # Bhattacharyya distance in range 0-1, where 0 IS THE MOST SIMILAR\n",
    "            # similarity = cv2.compareHist(test_hist, train_hist, cv2.HISTCMP_KL_DIV) # Kullback-Leibler divergence\n",
    "            similarities.append((similarity, train_img_idx))\n",
    "\n",
    "        # Convert to numpy array for sorting\n",
    "        similarities = np.array(similarities, dtype=[('similarity', float), ('index', int)])\n",
    "        # Sort by similarity\n",
    "        similarities.sort(order='similarity')\n",
    "        top_n = 3\n",
    "        # similarities = similarities[:top_n]\n",
    "\n",
    "        # display the results:\n",
    "        fig, axis = plt.subplots(1, top_n + 1)\n",
    "        fig.set_figwidth(10)\n",
    "        # query image\n",
    "        axis[0].title.set_text('Query')\n",
    "        axis[0].imshow(mpimg.imread(jpg_path))\n",
    "        # predictions\n",
    "        for i in range(top_n):\n",
    "            axis[i+1].title.set_text(f'{100 - round(similarities[i][0] * 100)}%')\n",
    "            axis[i+1].imshow(mpimg.imread(train_imgs[similarities[i][1]]))\n",
    "\n",
    "        # print(f'{jpg_path} top {top_n} similarities:', list(map(lambda x: (f'{round(x[0] * 100)}%', train_imgs[x[1]]), similarities)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This needs to be reworked. Idk how its supposed to work, this is mostly GPT code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF method\n",
    "\n",
    "# Precompute TF-IDF for each histogram\n",
    "idf = np.log(train_histograms / (np.count_nonzero(train_histograms > 0, axis=0) + 1))\n",
    "tf = train_histograms / train_histograms.sum(axis=1, keepdims=True)\n",
    "training_tfidf_vectors = tf * idf\n",
    "print(training_tfidf_vectors.shape)\n",
    "print(\"Test TF-IDF contains NaN:\", np.isnan(training_tfidf_vectors).any())\n",
    "print(training_tfidf_vectors)\n",
    "\n",
    "for jpg_path in test_img[:1]:\n",
    "    test_hist = GenDescriptorClusterHistogram(jpg_path)\n",
    "    if test_hist is not None:\n",
    "        # Compute TF for the test histogram\n",
    "        test_hist_tf = test_hist / np.sum(test_hist)\n",
    "\n",
    "        # Compute TF-IDF for the test histogram\n",
    "        test_hist_tfidf = test_hist_tf * idf\n",
    "        print(test_hist_tfidf.shape)\n",
    "        print(\"Test TF-IDF contains NaN:\", np.isnan(test_hist_tfidf).any())\n",
    "\n",
    "        # Compare with training histograms' TF-IDF vectors\n",
    "        similarities = cosine_similarity(test_hist_tfidf, training_tfidf_vectors)\n",
    "        print(similarities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
